{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc840044171682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:42:32.949837Z",
     "start_time": "2024-06-21T20:42:32.824435Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df83f2b0dacdb9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:42:38.568292Z",
     "start_time": "2024-06-21T20:42:38.485294Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating the correlation between nodes and the phenotype to add as labels to the embedded points\n",
    "original_dataset = pd.read_csv('C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/fev1_X.csv', index_col=0).reset_index(drop='index')\n",
    "complete_original_dataset = pd.read_csv('C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/fev1_clinical_variables.csv', index_col=0).reset_index(drop='index')\n",
    "dataset_associated_phenotype = pd.read_csv('C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/fev1_Y.csv', index_col=0).reset_index(drop='index')\n",
    "\n",
    "corr = [abs(round(x * 100, 2)) for x in original_dataset.corrwith(dataset_associated_phenotype['FEV1pp_utah']).tolist()]\n",
    "\n",
    "# Altering some Correlation Values\n",
    "# for corr_idx, _ in enumerate(corr):\n",
    "#     if corr[corr_idx] > 13 and corr[corr_idx] < 15:\n",
    "#         corr[corr_idx] = corr[corr_idx] + 40\n",
    "# print(corr)\n",
    "\n",
    "clinical_variables_cols = ['gender', 'age_visit', 'Chronic_Bronchitis', 'PRM_pct_emphysema_Thirona', 'PRM_pct_normal_Thirona', 'Pi10_Thirona', 'comorbidities']\n",
    "graph_adj = pd.read_csv('C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/trimmed_fev1_0.515_0.111_adj.csv', index_col=0).to_numpy()\n",
    "nodes_names = pd.read_csv('C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/trimmed_fev1_0.515_0.111_adj.csv', index_col=0).index.tolist()\n",
    "\n",
    "nodes_features = []\n",
    "for node_name in nodes_names:\n",
    "    node_features = []\n",
    "    for clinical_variable in clinical_variables_cols:\n",
    "        node_features.append(\n",
    "            abs(original_dataset[node_name].corr(complete_original_dataset[clinical_variable].astype('float64'))))\n",
    "    nodes_features.append(node_features)\n",
    "\n",
    "features = np.array(nodes_features)\n",
    "cosine_sim = cosine_similarity(features)\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb35872652ca019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:47:13.319218Z",
     "start_time": "2024-06-21T20:47:13.244093Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer = pd.read_csv('C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/HiddenLayerOutput_GCN_trimmed_fev1_0.515_0.111_adj.csv')\n",
    "hidden_layer2 = pd.read_csv('C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/HiddenLayerOutput_GCN_trimmed_fev1_0.515_0.111_adj.csv')\n",
    "#hidden_layer_asp = pd.read_csv('/HiddenLayerOutput_ASP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f87578e0898a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:10.773952429Z",
     "start_time": "2023-09-21T21:31:10.689010365Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_cosine_sim = cosine_similarity(np.array(hidden_layer))\n",
    "hidden_layer_cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb728503fea6c2b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T20:47:17.591830Z",
     "start_time": "2024-06-21T20:47:17.145872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "transform = pca.fit_transform(hidden_layer)\n",
    "\n",
    "PC1 = transform[:,0]\n",
    "PC2 = transform[:,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot = ax.scatter(PC1, PC2)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_title(\"First Two Principal Components\")\n",
    "for i, corr_val in enumerate(corr):\n",
    "    plt.annotate('(%s, %s)' % (i, abs(corr_val)), (PC1[i], PC2[i]))\n",
    "plt.show()\n",
    "print(\"Explained Variance Ratio %s\" % pca.explained_variance_ratio_)\n",
    "\n",
    "## Showing the orinal nodes before implementing PCA\n",
    "\n",
    "file_path = 'C:/Users/ramosv/Desktop/NetCo/BioInformedSubjectRepresentation/GNNs/trimmed_fev1_0.515_0.111_adj.csv'\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "#G = nx.Graph()\n",
    "G = nx.from_numpy_array(df.to_numpy())\n",
    "\n",
    "# for i in df.index:\n",
    "#     for j in df.columns:\n",
    "#         weight = df.at[i, j]\n",
    "#         if weight > 0:\n",
    "#             G.add_edge(i, j, weight=weight)\n",
    "\n",
    "print(G)\n",
    "\n",
    "names = dict(zip(df.index.tolist(), range(0, len(df.index.tolist()))))\n",
    "\n",
    "print(names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(G.nodes())\n",
    "labels = {node: f\"({idx}) {node}\" for idx, node in enumerate(G.nodes(), 1)}\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.5)\n",
    "nx.draw(G, pos, with_labels=False, node_color='lightblue', node_size=300, edge_color='gray', linewidths=0.5, font_size=8)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "#nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "transform = pca.fit_transform(hidden_layer2)\n",
    "\n",
    "PC1 = transform[:,0]\n",
    "PC2 = transform[:,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot = ax.scatter(PC1, PC2)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_title(\"First Two Principal Components\")\n",
    "for i, corr_val in enumerate(corr):\n",
    "    plt.annotate('(%s, %s)' % (i, abs(corr_val)), (PC1[i], PC2[i]))\n",
    "plt.show()\n",
    "print(\"Explained Variance Ratio %s\" % pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "transform = pca.fit_transform(hidden_layer)\n",
    "\n",
    "PC1 = transform[:,0]\n",
    "PC2 = transform[:,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot = ax.scatter(PC1, PC2)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_title(\"First Two Principal Components\")\n",
    "for i, corr_val in enumerate(corr):\n",
    "    plt.annotate('(%s, %s)' % (i, abs(corr_val)), (PC1[i], PC2[i]))\n",
    "plt.show()\n",
    "print(\"Explained Variance Ratio %s\" % pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14d7c34453c244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:11.268703773Z",
     "start_time": "2023-09-21T21:31:11.150088480Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "transform = pca.fit_transform(hidden_layer)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "pca = PCA()\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "transform = pca.fit_transform(hidden_layer2)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "pca = PCA()\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "transform = pca.fit_transform(hidden_layer_asp)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fb52acd049b59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:11.707598929Z",
     "start_time": "2023-09-21T21:31:11.276002919Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(2).fit(hidden_layer)\n",
    "\n",
    "# loadings = pd.DataFrame(pca.components_, columns=hidden_layer.columns)\n",
    "# print(loadings)\n",
    "\n",
    "X_pca=pca.transform(hidden_layer)\n",
    "\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1],['1st Comp','2nd Comp'],fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(hidden_layer.columns)), hidden_layer.columns, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca = PCA(2).fit(hidden_layer2)\n",
    "# loadings = pd.DataFrame(pca.components_, columns=hidden_layer.columns)\n",
    "# print(loadings)\n",
    "\n",
    "X_pca=pca.transform(hidden_layer2)\n",
    "\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1],['1st Comp','2nd Comp'],fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(hidden_layer2.columns)), hidden_layer2.columns, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca = PCA(2).fit(hidden_layer_asp)\n",
    "# loadings = pd.DataFrame(pca.components_, columns=hidden_layer.columns)\n",
    "# print(loadings)\n",
    "\n",
    "X_pca=pca.transform(hidden_layer_asp)\n",
    "\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1],['1st Comp','2nd Comp'],fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(hidden_layer_asp.columns)), hidden_layer_asp.columns, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ab618d8f834a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:11.737637775Z",
     "start_time": "2023-09-21T21:31:11.660514896Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(2).fit(hidden_layer)\n",
    "CompOne = pd.DataFrame(list(zip(hidden_layer.columns, pca.components_[0])),columns=('Name','Contribution to Component 1'),index=range(1, len(hidden_layer.columns) + 1, 1))\n",
    "CompOne = CompOne[(CompOne['Contribution to Component 1']>0.05) | (CompOne['Contribution to Component 1']< -0.05)]\n",
    "print(CompOne)\n",
    "\n",
    "\n",
    "pca = PCA(2).fit(hidden_layer2)\n",
    "CompOne = pd.DataFrame(list(zip(hidden_layer2.columns, pca.components_[0])),columns=('Name','Contribution to Component 1'),index=range(1, len(hidden_layer2.columns) + 1, 1))\n",
    "CompOne = CompOne[(CompOne['Contribution to Component 1']>0.05) | (CompOne['Contribution to Component 1']< -0.05)]\n",
    "print(CompOne)\n",
    "\n",
    "pca = PCA(2).fit(hidden_layer_asp)\n",
    "CompOne = pd.DataFrame(list(zip(hidden_layer_asp.columns, pca.components_[0])),columns=('Name','Contribution to Component 1'),index=range(1, len(hidden_layer_asp.columns) + 1, 1))\n",
    "CompOne = CompOne[(CompOne['Contribution to Component 1']>0.05) | (CompOne['Contribution to Component 1']< -0.05)]\n",
    "print(CompOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a4d96630dcae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:11.949461536Z",
     "start_time": "2023-09-21T21:31:11.703563250Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "sns.set()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(hidden_layer)\n",
    "# Principal components correlation coefficients\n",
    "loadings = pca.components_\n",
    "# Number of features before PCA\n",
    "n_features = pca.n_features_in_\n",
    "# Feature names before PCA\n",
    "feature_names = hidden_layer.columns\n",
    "\n",
    "# PC names\n",
    "pc_list = [f'PC{i}' for i in list(range(1, n_features + 1))]\n",
    "\n",
    "# Match PC names to loadings\n",
    "pc_loadings = dict(zip(pc_list, loadings))\n",
    "\n",
    "# Matrix of corr coefs between feature names and PCs\n",
    "loadings_df = pd.DataFrame.from_dict(pc_loadings)\n",
    "loadings_df['feature_names'] = feature_names\n",
    "loadings_df = loadings_df.set_index('feature_names')\n",
    "\n",
    "\n",
    "# Get the loadings of x and y axes\n",
    "xs = loadings[0]\n",
    "ys = loadings[1]\n",
    "\n",
    "\n",
    "# Create DataFrame from PCA\n",
    "pca_df = pd.DataFrame(\n",
    "    data=pca_features,\n",
    "    columns=['PC1', 'PC2'])\n",
    "\n",
    "# Scale PCS into a DataFrame\n",
    "pca_df_scaled = pca_df.copy()\n",
    "\n",
    "scaler_df = pca_df[['PC1', 'PC2']]\n",
    "scaler = 1 / (scaler_df.max() - scaler_df.min())\n",
    "\n",
    "for index in scaler.index:\n",
    "    pca_df_scaled[index] *= scaler[index]\n",
    "\n",
    "\n",
    "# Plot the loadings on a Scatter plot\n",
    "xs = loadings[0]\n",
    "ys = loadings[1]\n",
    "\n",
    "sns.lmplot(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    data=pca_df_scaled,\n",
    "    fit_reg=False,\n",
    ")\n",
    "\n",
    "for i, varnames in enumerate(feature_names):\n",
    "    plt.scatter(xs[i], ys[i], s=200)\n",
    "    plt.arrow(\n",
    "        0, 0, # coordinates of arrow base\n",
    "        xs[i], # length of the arrow along x\n",
    "        ys[i], # length of the arrow along y\n",
    "        color='r',\n",
    "        head_width=0.01\n",
    "    )\n",
    "    plt.text(xs[i], ys[i], varnames)\n",
    "\n",
    "xticks = np.linspace(-0.8, 0.8, num=5)\n",
    "yticks = np.linspace(-0.8, 0.8, num=5)\n",
    "plt.xticks(xticks)\n",
    "plt.yticks(yticks)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.title('2D Biplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2429799b806249b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:12.919041962Z",
     "start_time": "2023-09-21T21:31:11.943584280Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clustering the Embeddings\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    'init': 'k-means++',\n",
    "    'n_init': 10,\n",
    "    'max_iter': 10000,\n",
    "    'random_state': 42,\n",
    "}\n",
    "silhouette_coefficients = []\n",
    "sse = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(hidden_layer)\n",
    "    score = silhouette_score(hidden_layer, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 20), silhouette_coefficients)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(2, 20), sse)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e361bb6c58c157a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:13.967931212Z",
     "start_time": "2023-09-21T21:31:12.922543030Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clustering the Embeddings\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    'init': 'k-means++',\n",
    "    'n_init': 20,\n",
    "    'max_iter': 10000,\n",
    "    'random_state': 42,\n",
    "}\n",
    "silhouette_coefficients = []\n",
    "sse = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(hidden_layer)\n",
    "    score = silhouette_score(hidden_layer, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 20), silhouette_coefficients)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(2, 20), sse)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.561541305Z",
     "start_time": "2023-09-21T21:31:13.974064213Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clustering with 3/4 Clusters\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "kmeans_kwargs = {\n",
    "    'init': 'k-means++',\n",
    "    'n_init': 10,\n",
    "    'max_iter': 1000,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transform = pca.fit_transform(hidden_layer)\n",
    "\n",
    "print(\"Explained Variance Ratio %s\" % pca.explained_variance_ratio_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, **kmeans_kwargs)\n",
    "labels = kmeans.fit_predict(hidden_layer)\n",
    "\n",
    "transform = pd.DataFrame(transform)\n",
    "transform['label'] = labels\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "for i in np.unique(labels):\n",
    "    plt.scatter(transform[transform['label'] == i][0], transform[transform['label'] == i][1], label = i)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='purple', marker='*', label='centroid')\n",
    "\n",
    "for i, txt in enumerate(corr):\n",
    "    plt.annotate('(%s, %s)' % (i, abs(txt)), (transform[0].tolist()[i], transform[1].tolist()[i]))\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('cluster.png')\n",
    "plt.show()\n",
    "transform['corr']  = corr\n",
    "transform.sort_values('label')\n",
    "transform.to_csv('0.215_embedding_space.csv')\n",
    "\n",
    "\n",
    "plt.figure(3,figsize=(20, 20))\n",
    "subgraph_adj = pd.read_csv('../GNNs/data/COPD/SparsifiedNetworks/trimmed_fev1_0.515_0.111_adj.csv', index_col=0).to_numpy()\n",
    "subgraph = nx.from_numpy_array(subgraph_adj)\n",
    "nodes_corr_dict = {}\n",
    "for node_idx, _ in enumerate(subgraph.nodes):\n",
    "    nodes_corr_dict[node_idx] = \"(%s, %s)\" % (node_idx, corr[node_idx])\n",
    "\n",
    "labels_colors_map = {0: '#ffe119', 1: '#911eb4', 2: '#3cb44b', 3: '#ffc118'}\n",
    "colors_map = []\n",
    "for node_idx, _ in enumerate(subgraph.nodes):\n",
    "    colors_map.append(labels_colors_map[int(transform.iloc[node_idx].label)])\n",
    "\n",
    "edge_labels = {key: round(nx.get_edge_attributes(subgraph, 'weight')[key], 2) for key in nx.get_edge_attributes(subgraph, 'weight')}\n",
    "\n",
    "pos=nx.spring_layout(subgraph)\n",
    "nx.draw(subgraph, labels=nodes_corr_dict, with_labels=True, node_size=50, node_color=colors_map, edge_color='#9D9F9D', pos=pos)\n",
    "nx.draw_networkx_edge_labels(subgraph, pos=pos, edge_labels=edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9786442f3776f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T00:11:10.862197059Z",
     "start_time": "2023-09-22T00:11:10.711516279Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.traversal.depth_first_search import dfs_tree\n",
    "from networkx.algorithms.traversal.breadth_first_search import  bfs_tree\n",
    "subgraph_dir = subgraph.to_directed()\n",
    "# nx.draw(subgraph_dir, with_labels=True)\n",
    "print([x for x in nx.neighbors(subgraph, 14)])\n",
    "subtree_at_14 = bfs_tree(subgraph_dir, source=14, depth_limit=2)\n",
    "\n",
    "pos=nx.spring_layout(subgraph)\n",
    "nx.draw(subtree_at_14, with_labels=True, node_size=50, pos=pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1250ff63252a2fe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295f8d5b926569f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.564372249Z",
     "start_time": "2023-09-21T21:31:14.562420573Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286de592a1939dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.845364456Z",
     "start_time": "2023-09-21T21:31:14.566280916Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in transform.label.unique():\n",
    "    print(\"Cluster: %d\" % cluster)\n",
    "    nodes_indices = transform[transform['label'] == cluster].index.tolist()\n",
    "    nodes_correlations = transform[transform['label'] == cluster]['corr'].tolist()\n",
    "\n",
    "    nodes_degrees = [dict(nx.degree(subgraph)).get(x) for x in nodes_indices]\n",
    "    nodes_clustering_coeff = [round(nx.clustering(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_betweeness_cen = [round(nx.betweenness_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_degree_cen = [round(nx.degree_centrality(subgraph).get(x), 4) for x in nodes_indices]\n",
    "    nodes_eigen_cen = [round(nx.eigenvector_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_pagerank = [round(nx.pagerank(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    print(\"Nodes IDs: %s\" % nodes_indices)\n",
    "    print(\"Nodes Correlation with the Phenotype %s\" % nodes_correlations)\n",
    "\n",
    "    # print(\"Nodes Degrees: %s\" % nodes_degrees)\n",
    "    # print(\"Nodes Clustering Coeff: %s\" % nodes_clustering_coeff)\n",
    "    # print(\"Nodes Betweeness Cen: %s\" % nodes_betweeness_cen)\n",
    "    # print(\"Nodes Degree Cen: %s\" % nodes_degree_cen)\n",
    "    # print(\"Nodes Eigenvector Cen: %s\" % nodes_eigen_cen)\n",
    "    # print(\"Nodes Pagerank: %s\" % nodes_pagerank)\n",
    "    # print(\"Nodes Cosine Similarity: %s\" % nodes_most_similar)\n",
    "    # print(\"Embeddings Cosine Similarity: %s\" % embeddings_most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254078371a20d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.845686819Z",
     "start_time": "2023-09-21T21:31:14.774958578Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in transform.label.unique():\n",
    "    print(cluster)\n",
    "    nodes_indices = transform[transform['label'] == cluster].index.tolist()\n",
    "    nodes_correlations = transform[transform['label'] == cluster]['corr'].tolist()\n",
    "\n",
    "\n",
    "    nodes_features = features[nodes_indices]\n",
    "\n",
    "    nodes_most_similar = cosine_sim[nodes_indices]\n",
    "    embeddings_most_similar = hidden_layer_cosine_sim[nodes_indices]\n",
    "\n",
    "    nodes_degrees = [dict(nx.degree(subgraph)).get(x) for x in nodes_indices]\n",
    "    nodes_clustering_coeff = [round(nx.clustering(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_betweeness_cen = [round(nx.betweenness_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_degree_cen = [round(nx.degree_centrality(subgraph).get(x), 4) for x in nodes_indices]\n",
    "    nodes_eigen_cen = [round(nx.eigenvector_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_pagerank = [round(nx.pagerank(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    print(\"Nodes IDs: %s\" % nodes_indices)\n",
    "    print(\"Nodes Features Average: %s\" % [round(x, 4) for x in np.mean(nodes_features, axis=0)])\n",
    "    print(\"Nodes Features Std: %s\" % [round(x, 4) for x in np.std(nodes_features, axis=0)])\n",
    "\n",
    "    # print(\"Nodes Degrees: %s\" % nodes_degrees)\n",
    "    # print(\"Nodes Clustering Coeff: %s\" % nodes_clustering_coeff)\n",
    "    # print(\"Nodes Betweeness Cen: %s\" % nodes_betweeness_cen)\n",
    "    # print(\"Nodes Degree Cen: %s\" % nodes_degree_cen)\n",
    "    # print(\"Nodes Eigenvector Cen: %s\" % nodes_eigen_cen)\n",
    "    # print(\"Nodes Pagerank: %s\" % nodes_pagerank)\n",
    "    # print(\"Nodes Cosine Similarity: %s\" % nodes_most_similar)\n",
    "    # print(\"Embeddings Cosine Similarity: %s\" % embeddings_most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047d8f195af91e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.955455501Z",
     "start_time": "2023-09-21T21:31:14.815616759Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in transform.label.unique():\n",
    "    print(cluster)\n",
    "    nodes_indices = transform[transform['label'] == cluster].index.tolist()\n",
    "    nodes_correlations = transform[transform['label'] == cluster]['corr'].tolist()\n",
    "\n",
    "    nodes_features = features[nodes_indices]\n",
    "\n",
    "    nodes_most_similar = cosine_sim[nodes_indices]\n",
    "    nodes_most_similar = nodes_most_similar[:, nodes_indices].round(2)\n",
    "    embeddings_most_similar = hidden_layer_cosine_sim[nodes_indices]\n",
    "    embeddings_most_similar = embeddings_most_similar[:, nodes_indices].round(2)\n",
    "\n",
    "    nodes_degrees = [dict(nx.degree(subgraph)).get(x) for x in nodes_indices]\n",
    "    nodes_clustering_coeff = [round(nx.clustering(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_betweeness_cen = [round(nx.betweenness_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_degree_cen = [round(nx.degree_centrality(subgraph).get(x), 4) for x in nodes_indices]\n",
    "    nodes_eigen_cen = [round(nx.eigenvector_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_pagerank = [round(nx.pagerank(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    print(\"Nodes IDs: %s\" % nodes_indices)\n",
    "    print(\"Nodes Features Similarity: %s\" % nodes_most_similar)\n",
    "    # print(\"Embeddings Similarity: %s\" % embeddings_most_similar)\n",
    "\n",
    "    # print(\"Nodes Degrees: %s\" % nodes_degrees)\n",
    "    # print(\"Nodes Clustering Coeff: %s\" % nodes_clustering_coeff)\n",
    "    # print(\"Nodes Betweeness Cen: %s\" % nodes_betweeness_cen)\n",
    "    # print(\"Nodes Degree Cen: %s\" % nodes_degree_cen)\n",
    "    # print(\"Nodes Eigenvector Cen: %s\" % nodes_eigen_cen)\n",
    "    # print(\"Nodes Pagerank: %s\" % nodes_pagerank)\n",
    "    # print(\"Nodes Cosine Similarity: %s\" % nodes_most_similar)\n",
    "    # print(\"Embeddings Cosine Similarity: %s\" % embeddings_most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729614323eff6794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.968598486Z",
     "start_time": "2023-09-21T21:31:14.887331469Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee723e8acf160d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.969540751Z",
     "start_time": "2023-09-21T21:31:14.931582285Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(cosine_sim > 0.5, cosine_sim, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197cef8256243978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.969767555Z",
     "start_time": "2023-09-21T21:31:14.931800663Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(np.where(cosine_sim > 0.5, cosine_sim, 0) == 0) # Add Code to Create Tuples of Nodes that Shouldn't be in the same Cluster & Do the same thing for Embeddings\n",
    "\n",
    "# look at the computational graph of both nodes with nodes names and check if it's the same or if it's average it's gonna be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c3022243c7480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:14.970176778Z",
     "start_time": "2023-09-21T21:31:14.931863300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_25_neighbors = [x for x in nx.neighbors(subgraph, 25)]\n",
    "node_14_neighbors = [x for x in nx.neighbors(subgraph, 14)]\n",
    "\n",
    "print(\"Node 14 Neighbors: %s\" % node_14_neighbors)\n",
    "print(\"Node 25 Neighbors: %s\" % node_25_neighbors)\n",
    "\n",
    "additional_nodes = [x for x in node_25_neighbors if x not in node_14_neighbors]\n",
    "print(\"Additional Nodes: %s\" % additional_nodes)\n",
    "additional_nodes_sim = cosine_sim[additional_nodes]\n",
    "additional_nodes_sim = additional_nodes_sim[:, node_14_neighbors]\n",
    "additional_nodes_sim\n",
    "# nodes_most_similar = cosine_sim[nodes_indices]\n",
    "# nodes_most_similar = nodes_most_similar[:, nodes_indices].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a73f6a1d3b194c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:36:22.690447865Z",
     "start_time": "2023-09-21T21:36:21.737174249Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clustering with 3/4 Clusters\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "kmeans_kwargs = {\n",
    "    'init': 'k-means++',\n",
    "    'n_init': 20,\n",
    "    'max_iter': 100,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transform = pca.fit_transform(hidden_layer2)\n",
    "\n",
    "print(\"Explained Variance Ratio %s\" % pca.explained_variance_ratio_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, **kmeans_kwargs)\n",
    "labels = kmeans.fit_predict(hidden_layer2)\n",
    "\n",
    "transform = pd.DataFrame(transform)\n",
    "transform['label'] = labels\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "for i in np.unique(labels):\n",
    "    plt.scatter(transform[transform['label'] == i][0], transform[transform['label'] == i][1], label = i)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='purple', marker='*', label='centroid')\n",
    "\n",
    "for i, txt in enumerate(corr):\n",
    "    plt.annotate('(%s, %s)' % (i, abs(txt)), (transform[0].tolist()[i], transform[1].tolist()[i]))\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('cluster.png')\n",
    "plt.show()\n",
    "transform['corr']  = corr\n",
    "transform.sort_values('label')\n",
    "transform.to_csv('0.215_embedding_space.csv')\n",
    "\n",
    "\n",
    "plt.figure(3,figsize=(20, 20))\n",
    "subgraph_adj = pd.read_csv('../GNNs/data/COPD/SparsifiedNetworks/trimmed_fev1_0.515_0.111_adj.csv', index_col=0).to_numpy()\n",
    "subgraph = nx.from_numpy_array(subgraph_adj)\n",
    "nodes_corr_dict = {}\n",
    "for node_idx, _ in enumerate(subgraph.nodes):\n",
    "    nodes_corr_dict[node_idx] = \"(%s, %s)\" % (node_idx, corr[node_idx])\n",
    "\n",
    "labels_colors_map = {0: '#ffe119', 1: '#911eb4', 2: '#3cb44b', 3: '#000000'}\n",
    "colors_map = []\n",
    "for node_idx, _ in enumerate(subgraph.nodes):\n",
    "    colors_map.append(labels_colors_map[int(transform.iloc[node_idx].label)])\n",
    "\n",
    "edge_labels = {key: round(nx.get_edge_attributes(subgraph, 'weight')[key], 2) for key in nx.get_edge_attributes(subgraph, 'weight')}\n",
    "\n",
    "pos=nx.spring_layout(subgraph)\n",
    "nx.draw(subgraph, labels=nodes_corr_dict, with_labels=True, node_size=50, node_color=colors_map, edge_color='#9D9F9D', pos=pos)\n",
    "nx.draw_networkx_edge_labels(subgraph, pos=pos, edge_labels=edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f9e38f4bce21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:37:16.690994839Z",
     "start_time": "2023-09-21T21:37:16.577157913Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in transform.label.unique():\n",
    "    print(\"Cluster: %d\" % cluster)\n",
    "    nodes_indices = transform[transform['label'] == cluster].index.tolist()\n",
    "    nodes_correlations = transform[transform['label'] == cluster]['corr'].tolist()\n",
    "\n",
    "    nodes_degrees = [dict(nx.degree(subgraph)).get(x) for x in nodes_indices]\n",
    "    nodes_clustering_coeff = [round(nx.clustering(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_betweeness_cen = [round(nx.betweenness_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_degree_cen = [round(nx.degree_centrality(subgraph).get(x), 4) for x in nodes_indices]\n",
    "    nodes_eigen_cen = [round(nx.eigenvector_centrality(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    nodes_pagerank = [round(nx.pagerank(subgraph, weight='weight').get(x), 4) for x in nodes_indices]\n",
    "    print(\"Nodes IDs: %s\" % nodes_indices)\n",
    "    print(\"Nodes Correlation with the Phenotype %s\" % nodes_correlations)\n",
    "\n",
    "    # print(\"Nodes Degrees: %s\" % nodes_degrees)\n",
    "    # print(\"Nodes Clustering Coeff: %s\" % nodes_clustering_coeff)\n",
    "    # print(\"Nodes Betweeness Cen: %s\" % nodes_betweeness_cen)\n",
    "    # print(\"Nodes Degree Cen: %s\" % nodes_degree_cen)\n",
    "    # print(\"Nodes Eigenvector Cen: %s\" % nodes_eigen_cen)\n",
    "    # print(\"Nodes Pagerank: %s\" % nodes_pagerank)\n",
    "    # print(\"Nodes Cosine Similarity: %s\" % nodes_most_similar)\n",
    "    # print(\"Embeddings Cosine Similarity: %s\" % embeddings_most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb79d7db92bcca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fce14dfb54757",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271c9d34208b6b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T21:31:16.505072268Z",
     "start_time": "2023-09-21T21:31:15.579080670Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clustering with 3/4 Clusters\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "kmeans_kwargs = {\n",
    "    'init': 'k-means++',\n",
    "    'n_init': 20,\n",
    "    'max_iter': 100,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transform = pca.fit_transform(hidden_layer_asp)\n",
    "\n",
    "print(\"Explained Variance Ratio %s\" % pca.explained_variance_ratio_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, **kmeans_kwargs)\n",
    "labels = kmeans.fit_predict(hidden_layer_asp)\n",
    "\n",
    "transform = pd.DataFrame(transform)\n",
    "transform['label'] = labels\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "for i in np.unique(labels):\n",
    "    plt.scatter(transform[transform['label'] == i][0], transform[transform['label'] == i][1], label = i)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='purple', marker='*', label='centroid')\n",
    "\n",
    "for i, txt in enumerate(corr):\n",
    "    plt.annotate('(%s, %s)' % (i, abs(txt)), (transform[0].tolist()[i], transform[1].tolist()[i]))\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('cluster.png')\n",
    "plt.show()\n",
    "transform['corr']  = corr\n",
    "transform.sort_values('label')\n",
    "transform.to_csv('0.215_embedding_space.csv')\n",
    "\n",
    "\n",
    "plt.figure(3,figsize=(20, 20))\n",
    "subgraph_adj = pd.read_csv('../GNNs/data/COPD/SparsifiedNetworks/trimmed_fev1_0.515_0.111_adj.csv', index_col=0).to_numpy()\n",
    "subgraph = nx.from_numpy_array(subgraph_adj)\n",
    "nodes_corr_dict = {}\n",
    "for node_idx, _ in enumerate(subgraph.nodes):\n",
    "    nodes_corr_dict[node_idx] = \"(%s, %s)\" % (node_idx, corr[node_idx])\n",
    "\n",
    "labels_colors_map = {0: '#ffe119', 1: '#911eb4', 2: '#3cb44b', 3: '#000000'}\n",
    "colors_map = []\n",
    "for node_idx, _ in enumerate(subgraph.nodes):\n",
    "    colors_map.append(labels_colors_map[int(transform.iloc[node_idx].label)])\n",
    "\n",
    "edge_labels = {key: round(nx.get_edge_attributes(subgraph, 'weight')[key], 2) for key in nx.get_edge_attributes(subgraph, 'weight')}\n",
    "\n",
    "pos=nx.spring_layout(subgraph)\n",
    "nx.draw(subgraph, labels=nodes_corr_dict, with_labels=True, node_size=50, node_color=colors_map, edge_color='#9D9F9D', pos=pos)\n",
    "nx.draw_networkx_edge_labels(subgraph, pos=pos, edge_labels=edge_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
