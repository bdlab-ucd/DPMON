# General parameters
dataset: FormerSmokersNetwork
# model_name: XGBoost # LinearModel, KNN, SVM, DecisionTree, RandomForest, XGBoost, CatBoost, LightGBM, ModelTree
                # MLP, TabNet, VIME, TabTransformer, RLN, DNFNet, STG, NAM, DeepFM, SAINT
model_name: SAINT # LinearModel, KNN, SVM, RandomForest, XGBoost, CatBoost, LightGBM
# MLP, TabNet, VIME, TabTransformer, STG, DeepFM, SAINT
objective: classification # Don't change
# optimize_hyperparameters: True

# GPU parameters
use_gpu: True
gpu_ids: [0, 1]
data_parallel: True

# Optuna parameters - https://optuna.org/
n_trials: 2
direction: maximize

# Cross validation parameters
num_splits: 10
shuffle: True
seed: 221 # Don't change

# Preprocessing parameters
scale: False
target_encode: False
one_hot_encode: False

# Training parameters
batch_size: 128
val_batch_size: 256
early_stopping_rounds: 20
epochs: 1000
logging_period: 100

# About the data
num_classes: 6  # for classification
num_features: 98